# llama.cpp Server Parameters

> [!NOTE]
> This documentation was generated by ChatGPT and GitHub Copilot.

## Network & Server Setup

### `--host 127.0.0.1`
Binds the server to localhost only.
- ✅ Only your machine can access the API (no LAN or internet access)

### `--port 8080`
The TCP port the HTTP server listens on.
- ✅ Your API endpoint will be something like `http://127.0.0.1:8080`

## Model & Templating

### `--model '%~dp0models\%MODEL_FILE%'`
Path to the model file.
- `%~dp0` = directory of the running batch file (Windows syntax)
- `%MODEL_FILE%` = environment variable holding the model filename
- ✅ Makes the script portable

### `--jinja`
Enables Jinja2-style chat templates.
- ✅ Required for modern chat models (LLaMA 3, Qwen, Mistral-Instruct, etc.) so prompts are formatted correctly

## Hardware & Performance

### `--device Vulkan0`
Uses the first Vulkan-capable GPU.
- ✅ GPU acceleration without CUDA (great for AMD / Intel GPUs)

### `--ctx-size 16384`
Context window size (tokens).
- ✅ The model can "remember" up to 16k tokens of conversation/history

### `--threads -1`
Uses all available CPU threads automatically.
- ✅ Good default unless you're sharing the machine

## Batching & Parallelism ⚡

### `--batch-size 2048`
Maximum number of tokens processed in a single batch.
- ✅ Larger = better throughput, more VRAM usage

### `--ubatch-size 512`
Micro-batch size (sub-batches).
- ✅ Helps balance memory usage vs. speed on GPUs

### `--parallel 4`
Allows 4 concurrent inference slots.
- ✅ Multiple requests can be processed at once

### `--cont-batching`
Enables continuous batching.
- ✅ New requests can join an existing batch mid-generation → much higher throughput under load

## Prompt & Memory Optimizations

### `--cache-prompt`
Caches prompt tokens.
- ✅ Faster responses when prompts share common prefixes (system prompt, chat history)

### `--fit on`
Automatically adjusts internal memory usage to fit the available VRAM/RAM.
- ✅ Prevents crashes due to over-allocation

### `--flash-attn on`
Enables Flash Attention (if supported by the backend).
- ✅ Faster attention computation and lower memory usage for long contexts

## Server Behavior & Monitoring

### `--sleep-idle-seconds 300`
After 5 minutes of inactivity, the model goes idle.
- ✅ Frees resources and lowers power usage

### `--metrics`
Exposes performance metrics (tokens/sec, memory, etc.).
- ✅ Useful for monitoring and tuning

### `--verbose`
Enables detailed logging.
- ✅ Helpful for debugging, noisy for production

## Sampling & Generation Behavior

### `--repeat-penalty 1.0`
Penalty for repeating tokens.
- `1.0` = effectively off
- ✅ The model is free to repeat itself if it wants

### `--temp 0.7`
Temperature controls randomness.
- **Lower** = more deterministic
- **Higher** = more creative
- ✅ `0.7` is a balanced, commonly used value

### `--top-p 1.0`
Nucleus sampling cutoff.
- `1.0` = no cutoff
- ✅ All tokens remain eligible

### `--min-p 0.01`
Minimum probability threshold.
- ✅ Extremely unlikely tokens are filtered out, reducing garbage output